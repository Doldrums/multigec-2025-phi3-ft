model: "mlx-model"

train: true

data: "data"

seed: 1337

lora_layers: 64

batch_size: 2

iters: 4000

val_batches: 100

learning_rate: 1e-6

steps_per_report: 10

steps_per_eval: 200

resume_adapter_file: null

adapter_path: "adapters"

save_every: 100

test: false

test_batches: 100

max_seq_length: 9216

grad_checkpoint: true

lora_parameters:
  keys: ["self_attn.o_proj", "self_attn.qkv_proj"]
  rank: 128
  alpha: 256
  scale: 6.0
  dropout: 0.1
